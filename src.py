# for plotting in the iPython notebook
%matplotlib inline
# import necessary modules
import sys
import random
import time
import numpy as np
import matplotlib.pyplot as plt
from keras.layers import Input
from keras.models import Model, Sequential
from keras.layers.convolutional import UpSampling2D, Convolution2D
from keras.layers.normalization import BatchNormalization
from keras.layers.core import Reshape, Dense, Dropout, Flatten
from keras.layers.advanced_activations import LeakyReLU
from keras.datasets import mnist
from keras.optimizers import Adam, Nadam, SGD

class Generator:
    def __init__(self, dim):
        self.model = Sequential()
        self.model.add(Dense(128, input_dim=dim))
        self.model.add(LeakyReLU())
        self.model.add(Dense(256))
        self.model.add(LeakyReLU())
        self.model.add(Dense(512))
        self.model.add(LeakyReLU())
        self.model.add(Dense(1024))
        self.model.add(LeakyReLU())
        self.model.add(Dense(784, activation='tanh'))
        
        print(self.model.summary())
        opt = Adam(lr=0.0003, beta_1=0.6)
        self.model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['mae'])

class Discriminator:
    def __init__(self):
        self.model = Sequential()
        self.model.add(Dense(1024, input_dim=784))
        self.model.add(LeakyReLU())
        self.model.add(Dropout(0.3))
        self.model.add(Dense(512))
        self.model.add(LeakyReLU())
        self.model.add(Dropout(0.3))
        self.model.add(Dense(256))
        self.model.add(LeakyReLU())
        self.model.add(Dropout(0.3))
        self.model.add(Dense(128))
        self.model.add(LeakyReLU())
        self.model.add(Dropout(0.3))
        self.model.add(Dense(1, activation='sigmoid'))
        
        print(self.model.summary())
        opt = Adam(lr=0.0003, beta_1=0.6)
        self.model.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['mae'])

class GAN:
    def __init__(self):
        self.x_train = None
        self.dim = 10
        self.genModel = Generator( self.dim ).model
        self.discModel = Discriminator().model
        self.stackModel = None
        self.g_lossList = []
        self.d_lossList = []
        self.g_maeList = []
        self.d_maeList = []
    
    def combineGenDisc(self):
        self.discModel.trainable = False
        tempInput = Input(shape=(self.dim, ))
        temp = self.genModel( tempInput )
        tempOutput = self.discModel( temp )
        
        self.stackModel = Model( input = tempInput, output = tempOutput )
        print(self.stackModel.summary())
        opt = Adam(lr=0.0003, beta_1=0.6)
        self.stackModel.compile(loss = 'binary_crossentropy', optimizer = opt, metrics = ['mae'])
        
    def load_data(self):
        print("Loading MNIST dataset")
        (self.x_train, _), (x_test, _) = mnist.load_data()
        # concatenate the test data into train data, since we are not using test data
        self.x_train = np.concatenate((self.x_train, x_test))
        # normalize the training data before feeding them to our discriminator and generator
        self.x_train = (self.x_train.astype(np.float32) - 127.5)/127.5
        self.x_train = self.x_train.reshape(70000, 784)
        print(self.x_train.shape)
    
    def plotAcc(self):
        plt.figure(figsize=(15, 9))
        plt.plot(self.d_maeList, label='Discriminitive MAE', color='r')
        plt.plot(self.g_maeList, label='Generative MAE', color='g')
        plt.xlabel('Epoch')
        plt.ylabel('MAE')
        plt.legend()
        plt.show()
    
    def genPlot(self):
        noise = np.random.normal(0, 1, size=[10, self.dim])
        generatedImg = self.genModel.predict(noise)
        
        plt.figure(figsize=(20,4))
        for idx, i in enumerate(generatedImg):
            ax = plt.subplot(2, 10, idx + 1)
            plt.imshow(i.reshape(28, 28))
            plt.gray()
            ax.get_xaxis().set_visible(False)
            ax.get_yaxis().set_visible(False)
        plt.show()
    
    def train(self, maxEpoch, plotInterval=5, batchSize=128):
        batchNum = int(self.x_train.shape[0] / batchSize)

        for i in range(maxEpoch):
            print ("======= Epoch", (i+1), "=======")
            start = time.time()
            for j in range(batchNum):
                # get real images
                real_img = self.x_train[ random.sample(range( self.x_train.shape[0] ), batchSize) ]
                
                # get fake images generated by the generator from noise
                noise = np.random.normal(0, 1, size=[batchSize, self.dim])
                fake_img = self.genModel.predict(noise)
                X = np.concatenate((fake_img, real_img))
                
                # create the label for training
                y_0 = np.zeros(batchSize)
                # this is one-sided label soothing mentioned above
                # instead of directly using 1 as the label for a real image
                # I use random values between 0.95~1.05 as the labels
                y_1 = 0.1* np.random.random_sample(batchSize) + 0.95
                y = np.concatenate( (y_0, y_1) )

                # train the discriminator
                # set the "trainable" as True since we are going to train it
                self.discModel.trainable = True
                d_loss = self.discModel.train_on_batch(X, y)

                # noise is the training data for the generator
                noise = np.random.normal(0, 1, size=[2*batchSize, self.dim])
                # create the label for training
                y = np.ones(2*batchSize)
                
                # train the generator
                # set the "trainable" as False
                # because we should only train the generator here
                # we should not train the discriminator to meet the goal of the generator
                # remember that the discriminator and the generator have opposite goals
                self.discModel.trainable = False
                g_loss = self.stackModel.train_on_batch(noise, y)

            end = time.time()
            self.d_lossList.append(d_loss[0])
            self.g_lossList.append(g_loss[0])
            self.d_maeList.append(d_loss[1])
            self.g_maeList.append(g_loss[1])
            print(" - Training time:", (end-start), "sec")
            print(" - d_loss:", d_loss[0], " - g_loss:", g_loss[0])
            print(" - d_mae:", d_loss[1], " - g_mae:", g_loss[1])
            sys.stdout.flush()

            if i == 0 or (i+1) % plotInterval == 0:
                self.genPlot()

obj = GAN()
obj.load_data()
obj.combineGenDisc()
obj.train(50)
obj.plotAcc()